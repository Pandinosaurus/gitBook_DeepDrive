# Summary

## Introduction/Survy

* [ReadME](README.md)
* [List\_of\_DeepDrive](-toread-deepdrive-.md)
* [List\_of\_DeepDrive2](listof-deepdrive2.md)
* [Paper\_2017\_Survey\_CV4Vehicle \(50%\)](paper2017-survey.md)
* [Paper\_2017\_Overview of Services \(50%\)](paper2017-overview-of-services.md)
* [Paper\_2013\_Survey\_Vehicle Detection \(0%\)](paper2013-survey-vehicle-detection.md)
* [Paper\_2017\_Guide\_Segmetation\_AutomatedDriving \(100%\)](paper2017-segmetation-automateddriving.md)
* [Tutorial\_2017\_CVPR\_3D Deeplearning](tutorial2017-cvpr-3d-deeplearning.md)
* [Paper\_2016\_Sementic\_Fusion](paper2016-sementic-fusion.md)
* [Paper\_2017\_VoxelNet](paper2017-voxelnet.md)

## LowLevel CV

* [List\_Scene Flow](listscene-flow.md)
* [Intro\_Scene Flow](introscene-flow.md)
* [Paper\_2014\_\[M\] DMP-MSnet \(20%\)](paper2014-depthmap-prediction.md)
* [Paper\_2015\_\[M\] Depth\_Monocular](paper2015-depth-monocular.md)
* [Paper\_2017\_\[M\] UnsuperviseMD \(30%\)](paper2016-monocular-depth.md)
* [Paper\_2017\_\[M\] Semi\_MDE \(0%\)](paper2017-semi-mde.md)
* [Paper\_2017\_\[M\] Domain Independent MDE  \(30%\)](paper2017-domain-independent-mde.md)
* [Paper\_2015\_FlowNet1 \(10%\)](paper2015-flownet1.md)
* [Paper\_2017\_FlowNet2](paper2017-flownet2.md)
* [Paper\_2015\_\[S\]DispNet \(30%\)](paper2015-dispnet.md)
* [Paper\_2016\_\[S\]DL4SM \(30%\)](paper2016-dl4sm.md)
* [Paper\_2016\_\[S\]StereoFusion \(0%\)](paper2016-stereofusion.md)

## 2D CNN

* [Paper\_2016\_MultiNet \(0%\)](paper2016-multinet.md)
* [Paper\_2017\_SqueezeDet \(50%\)](paper2016-squeezedet.md)
* [Paper\_2016\_SegNet](paper2016-segnet.md)

## 2D Monocular Vision

* [----- Monocular Vision -----](-monocular-vision-.md)
* [Intro\_MonocularVision](introback-projection.md)
* [Papers\_국내 논문\_단일카메라 \(100%\)](paperdepth-from-single-image/paper2015-b2e8-c77c-ce74-ba54-b77c-2-c7a5-c758-c774-bbf8-c9c0.md)
  * [Report\_2017\_Study\_Monocular \(70%\)](report2017-monocular-3-cnnmethods.md)
  * [Paper\_2016\_Unify monocular detectors \(5%\)](paper2017-unify-monocular-detectors.md)
* [Paper\_2016\_Mono3D2016 \(70%\)](papermonocular-3d.md)
* [Paper\_2017\_Deep3DBox \(15%\)](paper2017-3d-bbox.md)
* [Paper\_2017\_J-MOD \(30%\)](paper2017-j-mod.md)

## 2D Stereo Vision

* [----- Stereo Vision -----](-stereo-vision-.md)
* [Intro\_StereoVision](introstereovision.md)
  * [Paper\_2014\_Know\_limit\_stereo](paper2014-know-limit-stereo.md)
* [Paper\_2017\_3DOP\_X Chen \(70%\)](paper2017-3d-object-proposals.md)

## 3D OverView

* [Intro\_3D CloudPoint](intro3d-cloudpoint.md)
* [Paper\_2017\_Sruvey\_3D data\(100%\)](paper2017-sruvey-3d-data.md)
* [Paper\_2013\_Survey\_2D\_3DShape Descriptor \(30%\)](paper2016-deep-learning-representation.md)

## 3D Volumetic based

* [Paper\_2015\_3D\_ShapeNet \(50%\)](paper2015-3d-shapenet.md)
* [Paper\_2015\_VoxNet \(70%\)](papervoxnet.md)
* [Paper\_2016\_V\_M CNNs \(70%\)](paper2016-volumetric-multiview-cnns.md)

## 3D Multiview based

* [----- Multiview based -----](-multiview-based-.md)
* [Paper\_2015\_MVCNN \(70%\)](paper2015-mvcnn.md)
* [Paper\_2015\_DeepPano \(70%\)](paper2015-deeppano.md)
* [Paper\_2016\_pMVCNN \(30%\)](paper2016-pairwisemvcnn.md)
* [Paper\_2016\_VeloFCN\_Bo Li \(70%\)](paper2016-velofcn4vd.md)
* [Paper\_2016\_3D GAN](paper2016-3d-gan.md)
* [Paper\_2017\_LoDNN\_Road Detection \(10%\)](paper2017-lodnnroad-detection.md)
* [Paper\_2017\_SqueezeSeg](paper2017-squeezeseg.md)
* [Paper\_2016\_FPNN](paper2016-fpnn.md)

## 3D Pointcloud based

* [----- Pointcloud based -----](-pointcloud-based-.md)
* [Paper\_2016\_PointNet \(50%\)](paper2016-pointnet.md)
  * [PointNet-pytorch](paper2016-pointnet/pointnet-pytorch.md)
* [Paper\_2016\_FCN4VD\_Bo Li  \(30%\)](paper3d-cnn.md)
* [Paper\_2017\_Vote3Deep \(50%\)](papervote3deep.md)
* [Paper\_2017\_SegCloud](paper2017-segcloud.md)

## Fusion

* [List to Read](-sensor-fusion-c774-b780-.md)
* [Intro\_Early\_Late\_Deep\_Fusion](paper2013-radar-fusion.md)
* [\[논문\] Sensor\_fusion\_paper](sensorfusion-paper.md)
  * [Paper\_2011\_LateFusion \(0%\)](paper2011latefusion.md)
  * [Paper\_2016\_FusionNet \(100%\)](paper2016-fusionnet.md)
  * [Paper\_2016\_DeepSlidingShape \(70%\)](paper2016-deepslidingshape.md)
  * [Paper\_2016\_FuseNet \(50%\)](paper2016-fusenet.md)
  * [Paper\_2016\_HHA-Fusion \(30%\)](paper2016-fusing-lidar-image-pedestrian.md)
  * [Paper\_2017\_Decision-Level Fusion \(50%\)](paper2017-decision-level-fusion.md)
  * [Paper\_2017\_VANET\_3D \(50%\)](paper2017-vanet-3d.md)
  * [Paper\_2017\_3DCNN\_DQN\_RNN](paper20173dcnndqn-rnn.md)
  * [라이다 및 비전 센서 융합 기반 장애물 검지 및 차량 인식](b77c-c774-b2e4-bc0f-be44-c804-c13c-c11c-c735-d569-ae30-bc18-c7a5-c560-bb3c-ac80-c9c0-bc0f-cc28-b7c9-c778-c2dd.md)
  * [2017\_SensorFusion\_Simualation\_TURTLEBOT](2017sensorfusion-simualation-turtlebot.md)
* [\[구현\_1\] Paper\_2017\_MV3D\(70%\)](papermultiview-3d-cnn.md)
  * [Paper\_2016\_Deeply\_Fused\_Nets](paper2016-deeply-fused-nets.md)
  * [Code\_MV3D](papermultiview-3d-cnn/codemv3d.md)
  * [code\_MV3D\_TF](papermultiview-3d-cnn/codemv3d-tf.md)
* [\[구현\_2\] Udacity\_Program](-sensor-fusion-c774-b780-/udacityprogram.md)
* [Report\_2017\_cs231n\_Dempster-Shafer \(50%\)](report2017-cs231n-dempster-shafer.md)
* [Paper\_2017\_Sensor Modality Fusion \(50%\)](paper2017-sensor-modality-fusion.md)

## 2D + Time \(Video\)

* [Paper\_2017\_DeepFeatureFlow \(0%\)](paper2017-deepfeatureflow.md)

## 참고

* [ref00\_Terms](ref00terms.md)
* [ref01\_Hardware](ref01hardware.md)
* [ref02\_Metrics](ref02metrics.md)
* [ref03\_Tracklet](ref03tracklet.md)
* ref04\_non-maxima suppression \(NMS\)
* [ref05-Read\_RGBD](pointcloud-data/readrgbd.md)

## Out of Scope

* [Papers\_OccupancyGridFusion](papersoccupancygridfusion.md)
* [Paper\_2016\_MSF-OG \(5%\)](paper2016-msf-og.md)
* [----- Octree based -----](-octree-based-.md)
* [Paper\_2017\_OctNet \(30%\)](paper2017-octnet.md)
* [Paper\_2016\_3D-R2N2](paper2016-3d-r2n2.md)
* [----- 3D Feature based -----](-feature-based-.md)
* [Paper\_2015\_DeepSD \(70%\)](paper2016-deep-learning-representation/paper2015-3d-deep-shape-descriptor.md)
* [Paper\_2015\_DL Representation ](paper2016-deep-learning-representation/paper2015-dl-representation.md)
* [Paper\_2015\_3DMesh\_Laveling](paper2016-deep-learning-representation/paper2015-3dmesh-laveling.md)
* [Paper\_2014\_AE3D shape Retrieval \(30%\)](paper2014-ae3d-shape-retrieval.md)
* [Paper\_2016\_PointNet3D \(50%\)](paper2016-pointnet3d.md)

## Project

* [Project\_2017\_Berkeley](project2017-berkeley.md)
* [Project\_2017\_iPRoBe Lab](project2017-iprobe-lab.md)
* [Project\_Autoware](projectautoware.md)
* [Project\_DiDi Competition](projectdidi-competition.md)
  * [Didi\_Getting\_Started](projectdidi-competition/didigetting-started.md)
  * [Didi\_Solution\_hengck23](projectdidi-competition/didisolution-hengck23.md)
  * [Didi\_Solution\_OmgTeam](projectdidi-competition/didisolution-omgteam.md)
  * [Didi\_Solution\_Timelaps](projectdidi-competition/didisolution-timelaps.md)
  * [Didi\_ref\_3D DataHandling](projectdidi-competition/didiref-3d-datahandling.md)
  * [Didi\_ref\_visualization](projectdidi-competition/didiref-visualization.md)

